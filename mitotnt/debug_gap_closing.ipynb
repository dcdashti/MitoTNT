{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e593515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import igraph as ig\n",
    "from tqdm.notebook import trange\n",
    "from scipy.optimize import linear_sum_assignment as lap_solver\n",
    "from fastdist import fastdist\n",
    "from network_tracking import local_graph_comparison_score, list_to_str\n",
    "\n",
    "work_dir = 'D:/Python Scripts/Mito/MitoTNT/test_data/'\n",
    "data_dir = work_dir+'mitograph/'\n",
    "input_dir = work_dir+'tracking_input/'\n",
    "if not os.path.isdir(input_dir):\n",
    "    os.mkdir(input_dir)\n",
    "\n",
    "start_frame = 0\n",
    "end_frame = len([frame for frame in os.listdir(data_dir) if 'FRAME' in frame])\n",
    "\n",
    "track_dir = work_dir+'tracking_ouput/'\n",
    "if not os.path.isdir(track_dir):\n",
    "    os.mkdir(track_dir)\n",
    "\n",
    "tracking_interval = 1\n",
    "graph_matching_depth = 2\n",
    "dist_exponent, top_exponent = 1, 1\n",
    "\n",
    "min_track_size = 4\n",
    "max_gap_size = 2\n",
    "memory_efficient_gap_closing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecfce1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gap closing in progress ...\n",
      "Data loading ...\n"
     ]
    }
   ],
   "source": [
    "## reload all tracks\n",
    "all_tracks = np.load(track_dir+'/all_tracks.npy', allow_pickle=True)\n",
    "\n",
    "# filter out very short tracks\n",
    "very_short_tracks = []\n",
    "for i in range(len(all_tracks)):\n",
    "    if len(all_tracks[i][0]) < min_track_size:\n",
    "        very_short_tracks.append(i)\n",
    "\n",
    "all_tracks = np.delete(all_tracks, very_short_tracks, axis=0)\n",
    "all_tracks = sorted(all_tracks, key=lambda track : track[0][0]) # sort by start frame\n",
    "num_tracks = len(all_tracks)\n",
    "\n",
    "print('Gap closing in progress ...')\n",
    "\n",
    "# get track disps\n",
    "all_track_disps = []\n",
    "for t in all_tracks:\n",
    "    track_coords = t[4] # use index for the coordinates\n",
    "    all_track_disps.append([np.linalg.norm(track_coords[t+1]-track_coords[t]) for t in range(len(track_coords)-1)])\n",
    "\n",
    "# store intermediate values\n",
    "print('Data loading ...')\n",
    "inputs = np.load(input_dir+'tracking_inputs.npz', allow_pickle=True)\n",
    "full_graph_all_frames = inputs['full_graphs']\n",
    "classic_graph_per_node_all_frames = inputs['classic_graphs_per_node']\n",
    "segment_node_all_frames = inputs['segment_nodes']\n",
    "\n",
    "all_track_assignments = {}\n",
    "partition_start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d0b037d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition index: 0 4260\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c32d94fa574440a970aa988af14993e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "276 MB \n",
      "\n",
      "Partition index: 3550 7810\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5428750824b34468be40cbc05a2ea1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "276 MB \n",
      "\n",
      "Partition index: 7100 11360\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5540e30c6874a9d9bbe40d66c651df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "276 MB \n",
      "\n",
      "Partition index: 10650 13225\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84cb14a5dbe040abb5f11c80bbd9da92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "276 MB \n",
      "\n"
     ]
    }
   ],
   "source": [
    "while partition_start < num_tracks:\n",
    "    num_frame = end_frame-start_frame\n",
    "\n",
    "    if not memory_efficient_gap_closing:\n",
    "        partition_size = num_tracks\n",
    "        overlap_size = 0\n",
    "    else:\n",
    "        partition_size = int(num_tracks / num_frame) * 30\n",
    "        overlap_size =  int(num_tracks / num_frame) * 5\n",
    "\n",
    "    partition_end = partition_start + partition_size\n",
    "    if partition_end > num_tracks:\n",
    "        partition_end = num_tracks\n",
    "    print('Partition index:', partition_start, partition_end)\n",
    "\n",
    "    track_cost_m_n = np.empty([partition_size, partition_size])\n",
    "    track_cost_m_n[:] = np.nan\n",
    "\n",
    "    for i in trange(partition_start, partition_end):\n",
    "\n",
    "        track_frames_m, track_nodes_m, track_coords_m = all_tracks[i][0], all_tracks[i][1], all_tracks[i][4]\n",
    "        end_frame_m, end_node_m, end_coord_m = track_frames_m[-1], track_nodes_m[-1], track_coords_m[-1]\n",
    "\n",
    "        # load data\n",
    "\n",
    "        classic_graphs_m = classic_graph_per_node_all_frames[end_frame_m]\n",
    "        disps_m = all_track_disps[i]\n",
    "\n",
    "        for j in range(i+1, partition_end): # no need to check index less than i because the start frame is sorted\n",
    "\n",
    "            track_frames_n, track_nodes_n, track_coords_n = all_tracks[j][0], all_tracks[j][1], all_tracks[j][4]\n",
    "            start_frame_n, start_node_n, start_coord_n = track_frames_n[0], track_nodes_n[0], track_coords_n[0]\n",
    "\n",
    "            gap_size = start_frame_n - end_frame_m - 1\n",
    "\n",
    "            # check only those within max gap size\n",
    "            if 1 <= gap_size <= max_gap_size: # if gap == 1 it should have been linked before - so skip it\n",
    "\n",
    "                # load data\n",
    "                classic_graphs_n = classic_graph_per_node_all_frames[start_frame_n]\n",
    "\n",
    "                # compute distance cutoff based on the two tracks\n",
    "                disps_n = all_track_disps[j]\n",
    "                comb_disps = disps_m + disps_n\n",
    "\n",
    "                dist_cutoff = (gap_size + 1) * (3 * np.std(comb_disps)) ** 2\n",
    "\n",
    "                # compute node-to-node distance\n",
    "                dist = end_coord_m - start_coord_n\n",
    "                dist_cost = np.sum(dist**2)\n",
    "\n",
    "                # filter by distance cutoff\n",
    "                if dist_cost > dist_cutoff:\n",
    "                    continue\n",
    "\n",
    "                # compute topology cost\n",
    "                topology_cost = local_graph_comparison_score(graph_matching_depth, end_node_m, start_node_n, classic_graphs_m, classic_graphs_n)\n",
    "\n",
    "                # assign g.c. cost\n",
    "                track_cost_m_n[i-partition_start,j-partition_start] = dist_cost**dist_exponent * topology_cost**top_exponent\n",
    "\n",
    "    # construct termination cost matrix\n",
    "    track_cost_m_m = np.empty([partition_size, partition_size])\n",
    "    track_cost_m_m[:] = np.nan\n",
    "\n",
    "    for i in range(partition_size):\n",
    "        row = track_cost_m_n[i,:]\n",
    "\n",
    "        if np.isnan(row).all():\n",
    "            track_cost_m_m[i,i] = 0 # must be assigned to itself since all other nodes exceed max radius\n",
    "        else:\n",
    "            min_cost = np.nanmin(row)\n",
    "            track_cost_m_m[i,i] = 1.5 * min_cost\n",
    "            # track_cost_m_m[i,i] = np.nanpercentile(row, 90, interpolation='midpoint')\n",
    "\n",
    "    # assemble into one matrix\n",
    "    track_cost_matrix = np.concatenate((track_cost_m_n, track_cost_m_m), axis=1)\n",
    "    track_cost_matrix[np.isnan(track_cost_matrix)] = np.inf\n",
    "\n",
    "    # evaluate memory usage\n",
    "    print('\\n%d MB' % (track_cost_matrix.nbytes / 1024**2), '\\n')\n",
    "\n",
    "    # solve LAP and store linking results\n",
    "    assignment = lap_solver(track_cost_matrix)[1]\n",
    "\n",
    "    linked, terminated, initiated = [], [], []\n",
    "    for i in range(len(assignment)):\n",
    "        if assignment[i] < partition_size:\n",
    "            linked.append([i, assignment[i]]) # first being index for frame t and second for frame t+tracking_interval\n",
    "        else:\n",
    "            terminated.append(i)\n",
    "\n",
    "    for pair in linked:\n",
    "        all_track_assignments[partition_start + pair[0]] = partition_start + pair[1] # offset by start index of the partition\n",
    "\n",
    "    # go to next partition\n",
    "    partition_start = partition_start + partition_size - overlap_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e2556b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dictionary to array and overwrite assignment by next partition's assignment for the overlapped region\n",
    "linked_tracks = np.zeros([len(all_track_assignments.keys()), 2], dtype=int)\n",
    "for i, a in enumerate(all_track_assignments.keys()):\n",
    "    linked_tracks[i,0] = a\n",
    "    linked_tracks[i,1] = all_track_assignments[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e600602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,  1458],\n",
       "       [    2,  1529],\n",
       "       [    3,  1483],\n",
       "       ...,\n",
       "       [12416, 13187],\n",
       "       [12419, 13223],\n",
       "       [12420, 13199]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linked_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06d12cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start combining closed tracks ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949e5cebfdf4447bab0e74847c232e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4696 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# combine tracks for gap closing\n",
    "print('Start combining closed tracks ...')\n",
    "if linked_tracks.shape[0] > 0:\n",
    "    linked_tracks_for_update = linked_tracks.copy() # used to record appended tracks\n",
    "    tracks_of_track = [] # list of linked tracks\n",
    "    all_linked_tracks = [] # record tracks that are closed\n",
    "\n",
    "    # recursive function for finding linked tracks\n",
    "    def find_all_linked_tracks(tracks, track_id):\n",
    "        for i in range(0, len(linked_tracks_for_update)):\n",
    "            if linked_tracks_for_update[i,0] == track_id: # find the first track\n",
    "                tracks.append(linked_tracks_for_update[i,1])\n",
    "                linked_tracks_for_update[i,0] = -1 # note that the track is already appended\n",
    "                find_all_linked_tracks(tracks, linked_tracks_for_update[i,1]) # go to the next track and find linked_tracks track\n",
    "\n",
    "    # for each track find the series of linked tracks\n",
    "    for index in trange(len(linked_tracks)):\n",
    "        track_id = linked_tracks[index,0]\n",
    "        if track_id in linked_tracks_for_update[:,0]:\n",
    "            tracks = [track_id]\n",
    "            find_all_linked_tracks(tracks, track_id)\n",
    "            tracks_of_track.append(tracks)\n",
    "            all_linked_tracks += tracks\n",
    "\n",
    "    # concatenate data for closed tracks\n",
    "    all_closed_tracks = []\n",
    "    for tot in tracks_of_track:\n",
    "        all_closed_tracks.append([sum([all_tracks[t][0] for t in tot], []),\n",
    "                                  sum([all_tracks[t][1] for t in tot], []),\n",
    "                                  sum([all_tracks[t][2] for t in tot], []),\n",
    "                                  sum([all_tracks[t][3] for t in tot], []),\n",
    "                                  sum([all_tracks[t][4] for t in tot], []),\n",
    "                                  sum([all_tracks[t][5] for t in tot], []),\n",
    "                                  sum([all_tracks[t][6] for t in tot], [])])\n",
    "\n",
    "    # add unclosed tracks back\n",
    "    for t in range(num_tracks):\n",
    "        if t not in all_linked_tracks:\n",
    "            all_closed_tracks.append(all_tracks[t].tolist())\n",
    "\n",
    "    # sort tracks\n",
    "    sort_by_length = sorted(all_closed_tracks, key=lambda track : len(track[0]), reverse=True) # first sort by size of track\n",
    "    sort_by_start = sorted(sort_by_length, key=lambda track : track[0][0]) # then sort by start frame\n",
    "    all_closed_tracks = sort_by_start\n",
    "\n",
    "else:\n",
    "    all_closed_tracks = all_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f704105",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18],\n",
       " [3, 3, 4, 1, 7, 10, 15, 11, 15, 13, 6, 5, 4, 1328, 7, 5, 7],\n",
       " [243,\n",
       "  268,\n",
       "  237,\n",
       "  271,\n",
       "  259,\n",
       "  266,\n",
       "  260,\n",
       "  258,\n",
       "  274,\n",
       "  268,\n",
       "  244,\n",
       "  274,\n",
       "  274,\n",
       "  262,\n",
       "  259,\n",
       "  259,\n",
       "  255],\n",
       " [3, 2, 3, 1, 5, 5, 5, 3, 1, 0, 5, 3, 2, 2, 1, 4, 0],\n",
       " [array([158., 125.,  42.]),\n",
       "  array([155., 124.,  42.]),\n",
       "  array([155., 122.,  42.]),\n",
       "  array([157., 121.,  43.]),\n",
       "  array([160., 122.,  42.]),\n",
       "  array([158., 127.,  41.]),\n",
       "  array([158., 128.,  41.]),\n",
       "  array([157., 129.,  41.]),\n",
       "  array([156., 129.,  41.]),\n",
       "  array([152., 129.,  40.]),\n",
       "  array([151., 129.,  42.]),\n",
       "  array([150., 125.,  42.]),\n",
       "  array([150., 123.,  42.]),\n",
       "  array([151., 124.,  42.]),\n",
       "  array([148., 126.,  41.]),\n",
       "  array([146., 127.,  42.]),\n",
       "  array([147., 127.,  41.])],\n",
       " [851.5,\n",
       "  1017.33333,\n",
       "  1406.66667,\n",
       "  923.0,\n",
       "  1921.33333,\n",
       "  2066.66667,\n",
       "  1721.5,\n",
       "  1152.5,\n",
       "  1813.0,\n",
       "  1071.0,\n",
       "  1651.33333,\n",
       "  1592.66667,\n",
       "  1305.5,\n",
       "  1499.66667,\n",
       "  1222.16667,\n",
       "  2050.0,\n",
       "  1371.33333],\n",
       " [1.2906,\n",
       "  1.86897,\n",
       "  2.32766,\n",
       "  2.14363,\n",
       "  3.06528,\n",
       "  2.9701,\n",
       "  2.53723,\n",
       "  2.11394,\n",
       "  3.25929,\n",
       "  1.9536,\n",
       "  2.82771,\n",
       "  2.79533,\n",
       "  2.2234,\n",
       "  3.20607,\n",
       "  2.37771,\n",
       "  2.84842,\n",
       "  3.0903]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot = tracks_of_track[0]\n",
    "[sum([all_tracks[t][0] for t in tot], []),\n",
    "                                  sum([all_tracks[t][1] for t in tot], []),\n",
    "                                  sum([all_tracks[t][2] for t in tot], []),\n",
    "                                  sum([all_tracks[t][3] for t in tot], []),\n",
    "                                  sum([all_tracks[t][4] for t in tot], []),\n",
    "                                  sum([all_tracks[t][5] for t in tot], []),\n",
    "                                  sum([all_tracks[t][6] for t in tot], [])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e33fc9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of tracks and average track length before gap closing: 13225 8.701701323251418\n",
      "\n",
      "Number of tracks and average track length after gap closing: 8529 13.492789307069996\n"
     ]
    }
   ],
   "source": [
    "np.save(track_dir+'all_closed_tracks.npy', np.array(all_closed_tracks, dtype=object))\n",
    "\n",
    "print('\\nNumber of tracks and average track length before gap closing:',\n",
    "      len(all_tracks), np.mean([len(track[0]) for track in all_tracks]))\n",
    "\n",
    "print('\\nNumber of tracks and average track length after gap closing:',\n",
    "      len(all_closed_tracks), np.mean([len(track[0]) for track in all_closed_tracks]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e0ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save tracks in the form of one node per row ###\n",
    "tracks = pd.DataFrame(columns={'frame_id', 'unique_node_id', 'frame_node_id', 'frame_seg_id', 'frame_frag_id', 'x', 'y', 'z','intensity','width'})\n",
    "tracks = tracks[['frame_id', 'unique_node_id', 'frame_node_id', 'frame_seg_id', 'frame_frag_id', 'x', 'y', 'z','intensity','width']] # reorder the columns\n",
    "\n",
    "df_index = 0\n",
    "for track_id, track in enumerate(all_closed_tracks):\n",
    "    track_frames, track_nodes, track_segs, track_frags, track_coords, track_ints, track_widths = track[0], track[1], track[2], track[3], track[4], track[5], track[6]\n",
    "\n",
    "    for f in range(len(track_frames)):\n",
    "        x, y, z = track_coords[f][0], track_coords[f][1], track_coords[f][2]\n",
    "        tracks.loc[df_index] = {'frame_id': track_frames[f], 'unique_node_id': track_id,\n",
    "                        'frame_node_id': track_nodes[f], 'frame_seg_id': track_segs[f], 'frame_frag_id': track_frags[f],\n",
    "                        'x': x, 'y': y, 'z': z, 'intensity': track_ints[f], 'width': track_widths[f]}\n",
    "        df_index += 1\n",
    "\n",
    "tracks.sort_values(['unique_node_id'], inplace=True, ignore_index=True)\n",
    "tracks.to_csv(track_dir+'tracks.csv', index=False)\n",
    "### Done saving ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a49a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save tracks and connected nodes using unique indexing ###\n",
    "new_tracks = pd.DataFrame()\n",
    "for frame in trange(start_frame, end_frame, tracking_interval):\n",
    "\n",
    "    full_graph = full_graph_all_frames[frame]\n",
    "    tracks_frame = tracks[tracks['frame_id'] == frame]\n",
    "\n",
    "    tracked_frame_nodes = tracks_frame['frame_node_id'].astype('int').tolist()\n",
    "    unique_nodes = tracks_frame['unique_node_id'].astype('int').tolist()\n",
    "    frame_to_unique = {tracked_frame_nodes[i]:unique_nodes[i] for i in range(len(tracks_frame))}\n",
    "\n",
    "    def find_connected_unique_nodes(this_node, visited_nodes):\n",
    "\n",
    "        neighs = full_graph.neighbors(this_node)\n",
    "        for visited_node in visited_nodes:\n",
    "            if visited_node in neighs:\n",
    "                neighs.remove(visited_node)\n",
    "\n",
    "        if len(neighs) == 0:\n",
    "            # print ('no neighbor for node',visited_nodes[0])\n",
    "            return\n",
    "\n",
    "        visited_nodes.append(this_node)\n",
    "\n",
    "        for neigh in neighs:\n",
    "            # if the frame node is tracked for this frame, add to list\n",
    "            if neigh in tracked_frame_nodes:\n",
    "                connected_unique_nodes.append(frame_to_unique[neigh])\n",
    "                # print('add',neigh,'for node',visited_nodes[0])\n",
    "            else:\n",
    "                find_connected_unique_nodes(neigh, visited_nodes)\n",
    "        return\n",
    "\n",
    "    all_connected_unique_nodes = []\n",
    "    for node in tracked_frame_nodes:\n",
    "        connected_unique_nodes = []\n",
    "        find_connected_unique_nodes(node, [node])\n",
    "        all_connected_unique_nodes.append(connected_unique_nodes)\n",
    "\n",
    "    tracks_frame.insert(2, 'connected_unique_node_id', [list_to_str(a) for a in all_connected_unique_nodes])\n",
    "    new_tracks = pd.concat([new_tracks, tracks_frame]) # accumulate tracks from each frame\n",
    "\n",
    "# reorder the columns\n",
    "new_tracks = new_tracks[['frame_id', 'unique_node_id', 'frame_node_id', 'frame_seg_id', 'frame_frag_id', 'connected_unique_node_id', 'x', 'y', 'z', 'intensity', 'width']]\n",
    "new_tracks.to_csv(track_dir+'tracks_with_connectivity_unique_index.csv', index=False)\n",
    "\n",
    "### Done saving ###\n",
    "\n",
    "\n",
    "### Save tracks and connected nodes using frame indexing ###\n",
    "new_tracks = pd.DataFrame()\n",
    "for frame in trange(start_frame, end_frame, tracking_interval):\n",
    "\n",
    "    full_graph = full_graph_all_frames[frame]\n",
    "    cc = full_graph.components()\n",
    "\n",
    "    segment_nodes = segment_node_all_frames[frame]\n",
    "    branching_nodes = []\n",
    "    for i in range(len(full_graph.vs)):\n",
    "        if full_graph.vs[i].degree() > 2:\n",
    "            branching_nodes.append(i)\n",
    "    node_to_segment = {}\n",
    "    for segment_id, segment in enumerate(segment_nodes): # segment consists of of segment nodes\n",
    "        for b in segment:\n",
    "            if b in branching_nodes:\n",
    "                node_to_segment[b] = np.nan\n",
    "            else:\n",
    "                node_to_segment[b] = segment_id\n",
    "\n",
    "    coords = full_graph.vs['coordinate']\n",
    "    intensities, widths = full_graph.vs['intensity'], full_graph.vs['width']\n",
    "    tracks_frame = tracks[tracks['frame_id'] == frame]\n",
    "\n",
    "    # add new rows for untracked nodes\n",
    "    untracked = []\n",
    "    tracked_frame_node_id = tracks_frame['frame_node_id'].astype('int').tolist()\n",
    "    for node in range(len(coords)):\n",
    "        if node not in tracked_frame_node_id:\n",
    "            coord = coords[node]\n",
    "            x, y, z = coord[0], coord[1], coord[2]\n",
    "            untracked.append({'frame_id': frame, 'unique_node_id': 'untracked',\n",
    "                              'frame_node_id': node, 'frame_seg_id': node_to_segment[node], 'frame_frag_id': cc.membership[node],\n",
    "                              'x': x, 'y': y, 'z': z,\n",
    "                              'intensity':intensities[node], 'width':widths[node]})\n",
    "\n",
    "    untracked_tracks = pd.DataFrame.from_dict(untracked)\n",
    "    tracks_frame = pd.concat([tracks_frame, untracked_tracks])\n",
    "\n",
    "    # add new column with neighbor nodes for each node\n",
    "    all_connected_frame_nodes = []\n",
    "    tracked_frame_node_id = tracks_frame['frame_node_id'].astype('int').tolist()\n",
    "    for node in tracked_frame_node_id:\n",
    "        neighs = full_graph.neighbors(int(node))\n",
    "        all_connected_frame_nodes.append(neighs)\n",
    "\n",
    "    tracks_frame.insert(2, 'connected_frame_node_id', [list_to_str(a) for a in all_connected_frame_nodes])\n",
    "\n",
    "    # accumulate tracks from each frame\n",
    "    new_tracks = pd.concat([new_tracks, tracks_frame])\n",
    "\n",
    "# reorder the columns\n",
    "new_tracks = new_tracks[['frame_id', 'unique_node_id', 'frame_node_id', 'frame_seg_id', 'frame_frag_id', 'connected_frame_node_id', 'x', 'y', 'z', 'intensity', 'width']]\n",
    "new_tracks.to_csv(track_dir+'tracks_with_connectivity_frame_index.csv', index=False)\n",
    "### Done saving ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
